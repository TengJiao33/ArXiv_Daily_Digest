"""
ArXiv Daily Digest â€” AI è®ºæ–‡ç²¾é€‰æ—¥æŠ¥
ä¸»æµç¨‹ï¼šæŠ“å– 50 ç¯‡ â†’ AI æ‰¹é‡ç­›é€‰ 9 ç¯‡ â†’ ä»£ç æ£€æŸ¥ â†’ Top2 æ·±åº¦æ‘˜è¦ â†’ åˆ†å±‚æŠ¥å‘Š â†’ å­˜å‚¨ â†’ æ¨é€
"""

import os
import json
import concurrent.futures
import io
import requests
import PyPDF2
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

from scraper_arxiv import ArxivScraper
from code_hunter import CodeHunter
from doubao_client import DoubaoClient
from notifier import HubNotifier
from storage import save_run_data


# â”€â”€â”€ é¢†åŸŸåˆ†ç±»æ˜ å°„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATEGORY_MAP = {
    "cs.CL": "è®¡ç®—è¯­è¨€å­¦",
    "cs.AI": "äººå·¥æ™ºèƒ½",
    "cs.LG": "æœºå™¨å­¦ä¹ ",
    "cs.CV": "è®¡ç®—æœºè§†è§‰",
    "cs.RO": "æœºå™¨äºº",
    "cs.SE": "è½¯ä»¶å·¥ç¨‹",
    "cs.CR": "åŠ å¯†ä¸å®‰å…¨",
    "cs.NE": "ç¥ç»ä¸è¿›åŒ–è®¡ç®—",
    "stat.ML": "ç»Ÿè®¡å­¦ä¹ ",
}

def get_category_name(cat_code):
    """è·å–åˆ†ç±»ä¸­æ–‡å"""
    primary = cat_code.split(".")[0] + "." + cat_code.split(".")[1] if "." in cat_code else cat_code
    cn = CATEGORY_MAP.get(primary, "")
    if cn:
        return f"{cat_code} ({cn})"
    return cat_code


# â”€â”€â”€ AI æ‰¹é‡ç­›é€‰æç¤ºè¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SELECTION_SYSTEM = """ä½ æ˜¯ä¸€ä½èµ„æ·± AI ç ”ç©¶é¡¾é—®ï¼Œæ¯å¤©å¸®åŠ©å¼€å‘è€…ä»æµ·é‡ ArXiv è®ºæ–‡ä¸­æŒ‘é€‰æœ€å€¼å¾—å…³æ³¨çš„ç ”ç©¶ã€‚
ä½ çš„åˆ¤æ–­æ ‡å‡†ï¼šåˆ›æ–°æ€§ã€å®ç”¨æ€§ã€å¯¹å·¥ä¸šç•Œçš„å½±å“åŠ›ã€ç ”ç©¶æ–¹å‘çš„å‰æ²¿ç¨‹åº¦ã€‚
ä½ å¯¹ LLMã€Agentã€RAGã€å¤šæ¨¡æ€ã€æ¨ç†ã€å¯¹é½ç­‰çƒ­é—¨æ–¹å‘æœ‰æ·±å…¥ç†è§£ã€‚"""

SELECTION_PROMPT = """ä»¥ä¸‹æ˜¯ä»Šå¤© ArXiv ä¸Šçš„ {count} ç¯‡ AI ç›¸å…³æ–°è®ºæ–‡ã€‚
è¯·ä»ä¸­ç²¾é€‰å‡º **æœ€å€¼å¾—å…³æ³¨çš„ 9 ç¯‡**ï¼ŒæŒ‰æ¨èç¨‹åº¦æ’åºï¼ˆæœ€æ¨èçš„æ’ç¬¬ä¸€ï¼‰ã€‚

è®ºæ–‡åˆ—è¡¨ï¼š
{papers_text}

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼ˆä¸è¦è¿”å›ä»»ä½•å…¶ä»–å†…å®¹ï¼‰ï¼š
```json
[
  {{"index": 0, "reason": "æ¨èç†ç”±ï¼ˆä¸­æ–‡ï¼Œ50-80å­—ï¼Œè¯¦ç»†è¯´æ˜è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Œæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼‰"}},
  {{"index": 1, "reason": "æ¨èç†ç”±..."}},
  ...
]
```

æ³¨æ„ï¼š
- index æ˜¯è®ºæ–‡åœ¨ä¸Šé¢åˆ—è¡¨ä¸­çš„ç¼–å·ï¼ˆä» 0 å¼€å§‹ï¼‰
- è¿”å›æ°å¥½ 9 ä¸ª
- reason ç”¨ä¸­æ–‡ï¼Œè¯¦ç»†æ¦‚æ‹¬æ¨èç†ç”±ï¼Œ50-80å­—
- ä¼˜å…ˆé€‰æ‹©ï¼šæœ‰çªç ´æ€§åˆ›æ–°çš„ã€è§£å†³å®é™…ç—›ç‚¹çš„ã€å¯èƒ½å¼•é¢†æ–°æ–¹å‘çš„"""


# â”€â”€â”€ AI æ·±åº¦æ‘˜è¦æç¤ºè¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DEEP_SUMMARY_SYSTEM = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI ç ”ç©¶å‘˜ï¼Œæ“…é•¿å°†å­¦æœ¯è®ºæ–‡è½¬åŒ–ä¸ºé€šä¿—æ˜“æ‡‚çš„ä¸­æ–‡æŠ€æœ¯è§£è¯»ã€‚
ä½ çš„è¯»è€…æ˜¯æœ‰æŠ€æœ¯èƒŒæ™¯çš„å¼€å‘è€…ï¼Œæƒ³å¿«é€Ÿäº†è§£è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ã€‚
è¾“å‡ºé£æ ¼ï¼šä¿¡æ¯å¯†åº¦é«˜ã€æœ‰æ´å¯ŸåŠ›ã€ä¸è¯´åºŸè¯ã€‚"""



DEEP_SUMMARY_PROMPT = """è¯·ä¸ºä»¥ä¸‹ ArXiv è®ºæ–‡æ’°å†™ä¸€æ®µæ·±åº¦æŠ€æœ¯è§£è¯»ã€‚

---
æ ‡é¢˜: {title}
ä½œè€…: {authors}
åˆ†ç±»: {category}
{code_info}

è®ºæ–‡å…¨æ–‡å†…å®¹ (å·²æå–æ–‡æœ¬):
{full_text}
---

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä¸¥æ ¼éµå®ˆæ ¼å¼ï¼‰ï¼š

**ä¸­æ–‡æ ‡é¢˜**: ï¼ˆç¿»è¯‘é¢˜ç›®ï¼‰

**èƒŒæ™¯ä¸ç—›ç‚¹**: ï¼ˆè¯¥ç ”ç©¶é’ˆå¯¹ä»€ä¹ˆé—®é¢˜ï¼Ÿç°æœ‰æ–¹æ³•æœ‰ä½•ç¼ºé™·ï¼Ÿ100å­—å·¦å³ï¼‰

**æ ¸å¿ƒåˆ›æ–°**: ï¼ˆæ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿç›¸æ¯”ç°æœ‰æŠ€æœ¯æœ‰ä½•ç‹¬ç‰¹ä¹‹å¤„ï¼Ÿ100å­—å·¦å³ï¼‰

**æŠ€æœ¯ç»†èŠ‚**: ï¼ˆå…·ä½“æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿæ¨¡å‹æ¶æ„ã€ç®—æ³•æµç¨‹ã€å…³é”®æŠ€æœ¯ç‚¹ã€‚150å­—å·¦å³ï¼Œè¶Šè¯¦ç»†è¶Šå¥½ï¼‰

**å®éªŒç»“æœ**: ï¼ˆåœ¨ä»€ä¹ˆæ•°æ®é›†ä¸Šæµ‹è¯•ï¼Ÿæ€§èƒ½æå‡å¤šå°‘ï¼Ÿæœ‰æ— å…³é”®ç»“è®ºï¼Ÿ50-100å­—ï¼‰

æ³¨æ„ï¼š
- æ·±åº¦è§£è¯»ï¼Œä¸è¦æ³›æ³›è€Œè°ˆï¼Œæ€»å­—æ•° 400-600 å­—
- ç”¨ä¸­æ–‡è¾“å‡º
- ç¦æ­¢ä½¿ç”¨ LaTeX å…¬å¼ï¼ˆæ‰‹æœºç«¯æ— æ³•æ¸²æŸ“ï¼‰ï¼Œæ‰€æœ‰æ•°å­¦å…¬å¼è¯·ç”¨çº¯æ–‡æœ¬æè¿°ï¼ˆå¦‚ï¼šå¹³æ–¹æ ¹ã€æ±‚å’Œã€y=f(x) ç­‰ï¼‰
- ä¿æŒä¸“ä¸šæ€§ï¼Œç»“æ„æ¸…æ™°ï¼Œä¸è¦ä½¿ç”¨ Markdown æ ‡é¢˜ï¼ˆ# ## ç­‰ï¼‰ï¼Œä½¿ç”¨åŠ ç²—æ ‡æ³¨å°æ ‡é¢˜"""


# â”€â”€â”€ é˜¶æ®µä¸€ï¼šAI æ‰¹é‡ç­›é€‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def ai_select_papers(papers, client):
    """
    å°†æ‰€æœ‰è®ºæ–‡æ ‡é¢˜+æ‘˜è¦æ‰“åŒ…å‘ç»™ AIï¼Œè®©å®ƒæŒ‘é€‰æœ€å€¼å¾—å…³æ³¨çš„ 13 ç¯‡ã€‚
    åªéœ€ 1 æ¬¡ API è°ƒç”¨ã€‚
    è¿”å›: (selected_papers, usage)
    """
    # æ„å»ºè®ºæ–‡åˆ—è¡¨æ–‡æœ¬
    paper_lines = []
    for i, p in enumerate(papers):
        # åªå–æ‘˜è¦å‰ 200 å­—èŠ‚çœ token
        abstract_short = p["summary"][:200]
        paper_lines.append(f"[{i}] ã€{p['category']}ã€‘{p['title']}\n    æ‘˜è¦: {abstract_short}...")

    papers_text = "\n\n".join(paper_lines)

    prompt = SELECTION_PROMPT.format(count=len(papers), papers_text=papers_text)

    print(f"[AI ç­›é€‰] æ­£åœ¨ä» {len(papers)} ç¯‡ä¸­æŒ‘é€‰ 9 ç¯‡...")
    response, usage = client.chat_completion(
        messages=[{"role": "user", "content": prompt}],
        system_prompt=SELECTION_SYSTEM,
        max_tokens=1024,
    )

    if not response:
        print("[AI ç­›é€‰] âŒ AI è°ƒç”¨å¤±è´¥ï¼Œéšæœºå–å‰ 9 ç¯‡ä½œä¸ºå…œåº•")
        return papers[:9], usage

    # è§£æ JSON å“åº”
    try:
        # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—åŒ…è£¹
        clean = response.strip()
        if clean.startswith("```"):
            clean = clean.split("\n", 1)[1]  # å»æ‰ç¬¬ä¸€è¡Œ ```json
            clean = clean.rsplit("```", 1)[0]  # å»æ‰æœ€åçš„ ```

        selections = json.loads(clean)
        selected = []
        seen_indices = set()

        for item in selections:
            idx = item.get("index", -1)
            if 0 <= idx < len(papers) and idx not in seen_indices:
                seen_indices.add(idx)
                paper = dict(papers[idx])
                paper["one_liner"] = item.get("reason", "")
                selected.append(paper)

        if len(selected) < 9:
            print(f"[AI ç­›é€‰] âš ï¸ åªè§£æåˆ° {len(selected)} ç¯‡ï¼Œè¡¥å……åˆ° 9 ç¯‡")
            for i, p in enumerate(papers):
                if i not in seen_indices and len(selected) < 9:
                    paper = dict(p)
                    paper["one_liner"] = "AI æœªæä¾›æ¨èç†ç”±"
                    selected.append(paper)

        print(f"[AI ç­›é€‰] âœ… ç²¾é€‰ {len(selected)} ç¯‡è®ºæ–‡")
        return selected, usage

    except (json.JSONDecodeError, KeyError, TypeError) as e:
        print(f"[AI ç­›é€‰] âš ï¸ JSON è§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨å‰ 9 ç¯‡")
        fallback = []
        for p in papers[:9]:
            paper = dict(p)
            paper["one_liner"] = "AI è§£æå¼‚å¸¸"
            fallback.append(paper)
        return fallback, usage


# â”€â”€â”€ é˜¶æ®µäºŒï¼šä»£ç æ£€æŸ¥ + æ·±åº¦æ‘˜è¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def check_all_code(papers, hunter):
    """å¹¶è¡Œæ£€æŸ¥æ‰€æœ‰è®ºæ–‡æ˜¯å¦æœ‰ GitHub ä»£ç """
    print(f"[ä»£ç æ£€æŸ¥] æ£€æŸ¥ {len(papers)} ç¯‡è®ºæ–‡çš„ä»£ç ä»“åº“...")

    def _check(paper):
        has_code, info = hunter.check_paper(paper)
        paper["has_code"] = has_code
        paper["repo_url"] = info.get("url", "") if has_code else ""
        paper["repo_stars"] = info.get("stars", 0) if has_code else 0
        return paper

    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(_check, p) for p in papers]
        results = [f.result() for f in futures]

    code_count = sum(1 for p in results if p["has_code"])
    print(f"[ä»£ç æ£€æŸ¥] âœ… {code_count}/{len(results)} ç¯‡è®ºæ–‡é™„å¸¦ä»£ç ä»“åº“")
    return results


def extract_full_text(pdf_url):
    """ä¸‹è½½ PDF å¹¶æå–å…¨æ–‡"""
    print(f"[PDF ä¸‹è½½] æ­£åœ¨è·å–: {pdf_url}")
    try:
        # ä¼ªè£… User-Agent é˜²æ­¢è¢«æ‹¦æˆª
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        # å°† arxiv.org æ›¿æ¢ä¸º export.arxiv.org å¯èƒ½æ›´ç¨³å®šï¼Œæˆ–è€…ä½¿ç”¨å›½å†…é•œåƒ
        # è¿™é‡Œå…ˆå°è¯•ç›´æ¥ä¸‹è½½ï¼Œè¶…æ—¶è®¾ç½®ç¨é•¿
        response = requests.get(pdf_url, headers=headers, timeout=30)
        response.raise_for_status()

        with io.BytesIO(response.content) as f:
            reader = PyPDF2.PdfReader(f)
            text = ""
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"
            
            # ç®€å•æ¸…æ´—
            text = text.strip()
            if len(text) < 500:
                print(f"[PDF è§£æ] âš ï¸ æå–æ–‡æœ¬è¿‡çŸ­ ({len(text)} å­—ç¬¦)ï¼Œå¯èƒ½è§£æå¤±è´¥")
                return None
            
            print(f"[PDF è§£æ] âœ… æˆåŠŸæå– {len(text)} å­—ç¬¦")
            return text

    except Exception as e:
        print(f"[PDF ä¸‹ä¸‹è½½/è§£æ] âŒ å¤±è´¥: {e}")
        return None


def generate_deep_summaries(top_papers, client):
    """å¯¹ Top 2 è®ºæ–‡ç”Ÿæˆæ·±åº¦ AI æ‘˜è¦"""
    total_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0, "api_calls": 0}

    for p in top_papers:
        # æ„å»ºä»£ç ä¿¡æ¯
        if p.get("has_code"):
            code_info = f"GitHub ä»£ç : {p['repo_url']} (â­ {p['repo_stars']})"
        else:
            code_info = "GitHub ä»£ç : æš‚æ— å…¬å¼€ä»£ç "


        # å°è¯•æå–å…¨æ–‡
        full_text = None
        if p.get("pdf_url"):
            full_text = extract_full_text(p["pdf_url"])
        
        # å¦‚æœæå–å¤±è´¥ï¼Œå›é€€åˆ°æ‘˜è¦
        if not full_text:
            content_input = f"æ‘˜è¦ (è‹±æ–‡):\n{p['summary']}"
            print(f"[æ·±åº¦æ‘˜è¦] âš ï¸ ä½¿ç”¨æ‘˜è¦å›é€€æ¨¡å¼")
        else:
            # é™åˆ¶æœ€å¤§é•¿åº¦é˜²æ­¢è¶…é•¿ (Doubao 32k/128k, è®¾ç½® 10w å­—ç¬¦ä¸€èˆ¬å®‰å…¨)
            if len(full_text) > 100000:
                full_text = full_text[:100000] + "\n...(åæ–‡æˆªæ–­)"
            content_input = full_text

        prompt = DEEP_SUMMARY_PROMPT.format(
            title=p["title"],
            authors=", ".join(p.get("authors", [])[:5]),
            category=get_category_name(p.get("category", "")),
            code_info=code_info,
            full_text=content_input,
        )

        try:
            response, usage = client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                system_prompt=DEEP_SUMMARY_SYSTEM,
                max_tokens=600,
            )
            if response:
                p["deep_summary"] = response
                print(f"[æ·±åº¦æ‘˜è¦] âœ… {p['title'][:40]}...")
                if usage:
                    total_usage["prompt_tokens"] += usage.get("prompt_tokens", 0)
                    total_usage["completion_tokens"] += usage.get("completion_tokens", 0)
                    total_usage["total_tokens"] += usage.get("total_tokens", 0)
                    total_usage["api_calls"] += 1
            else:
                p["deep_summary"] = f"_AI å·æ‡’äº†ï¼Œè¯·ç‚¹å‡»åŸé“¾æ¥æŸ¥çœ‹ ğŸ‘‰ [è®ºæ–‡]({p['url']})_"
        except Exception as e:
            print(f"[æ·±åº¦æ‘˜è¦] âŒ {p['title'][:30]}... å¤±è´¥: {e}")
            p["deep_summary"] = f"_AI å·æ‡’äº†ï¼Œè¯·ç‚¹å‡»åŸé“¾æ¥æŸ¥çœ‹ ğŸ‘‰ [è®ºæ–‡]({p['url']})_"

    return total_usage


# â”€â”€â”€ æŠ¥å‘Šç»„è£… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _code_badge(paper):
    """ç”Ÿæˆä»£ç å¾½ç« """
    if paper.get("has_code"):
        stars = paper.get("repo_stars", 0)
        url = paper.get("repo_url", "")
        return f"âœ… [ä»£ç ]({url}) â­{stars}"
    return "âš ï¸ æš‚æ— ä»£ç "


def build_report(selected_papers, usage_select, usage_deep, total_scanned):
    """ç»„è£…åˆ†å±‚ Markdown æŠ¥å‘Šï¼šTop 2 æ·±åº¦è§£è¯» + 7 ç¯‡é€Ÿè§ˆè¡¨æ ¼"""
    date_str = datetime.now().strftime("%Y-%m-%d")
    weekday_map = {0: "å‘¨ä¸€", 1: "å‘¨äºŒ", 2: "å‘¨ä¸‰", 3: "å‘¨å››", 4: "å‘¨äº”", 5: "å‘¨å…­", 6: "å‘¨æ—¥"}
    weekday = weekday_map[datetime.now().weekday()]

    # åˆå¹¶ç”¨é‡
    total_tokens = (usage_select.get("total_tokens", 0) + usage_deep.get("total_tokens", 0))
    total_calls = (usage_select.get("api_calls", 0) + usage_deep.get("api_calls", 0))

    lines = []
    lines.append(f"# ğŸ§ª ArXiv AI æ—¥æŠ¥\n")
    lines.append(f"ğŸ“… **{date_str} {weekday}** | ğŸ¤– æ‰«æ/ç²¾é€‰: **{total_scanned}/{len(selected_papers)}**\n")

    if total_tokens:
        prompt_t = usage_select.get("prompt_tokens", 0) + usage_deep.get("prompt_tokens", 0)
        comp_t = usage_select.get("completion_tokens", 0) + usage_deep.get("completion_tokens", 0)
        cost = prompt_t / 1_000_000 * 0.3 + comp_t / 1_000_000 * 0.6
        lines.append(f"> ğŸ“Š Tokens: **{total_tokens:,}** (Â¥{cost:.4f})\n")

    # â”€â”€â”€ ä»Šæ—¥å¿…è¯» (Top 2) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    top_papers = selected_papers[:2]
    rest_papers = selected_papers[2:]

    lines.append(f"## ğŸ”¥ ä»Šæ—¥å¿…è¯»\n")

    for i, p in enumerate(top_papers, 1):
        lines.append(f"---\n")
        lines.append(f"### {i}. {p['title']}\n")
        
        # é¡¶éƒ¨è§’æ ‡è¡Œ
        meta_parts = []
        cat_display = get_category_name(p.get('category', ''))
        meta_parts.append(f"ğŸ·ï¸ `{cat_display}`")
        meta_parts.append(f"ğŸ“„ [arXiv]({p['url']})")
        
        if p.get("has_code"):
             meta_parts.append(f"ğŸ’» [GitHub]({p.get('repo_url', '')}) â­{p.get('repo_stars', 0)}")
        
        lines.append(f"{' | '.join(meta_parts)}\n")

        authors = p.get("authors", [])
        if authors:
            author_str = ", ".join(authors[:3])
            if len(authors) > 3:
                author_str += f" ç­‰"
            lines.append(f"ğŸ‘¤ {author_str}\n")

        if p.get("deep_summary"):
            lines.append(f"\n{p['deep_summary']}\n")
        elif p.get("one_liner"):
             # AI ç”Ÿæˆæ·±åº¦æ‘˜è¦å¤±è´¥æ—¶çš„å…œåº•
            lines.append(f"\n> ğŸ’¬ {p['one_liner']}\n")

    # â”€â”€â”€ åŒæ ·å€¼å¾—å…³æ³¨ (è¡¨æ ¼é€Ÿè§ˆ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â”€â”€â”€ åŒæ ·å€¼å¾—å…³æ³¨ (è¡¨æ ¼é€Ÿè§ˆ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if rest_papers:
        lines.append(f"\n---\n")
        lines.append(f"## ğŸ“‹ åŒæ ·å€¼å¾—å…³æ³¨\n")
        lines.append(f"| # | è®ºæ–‡ | æ¨èç†ç”± |")
        lines.append(f"|:-:|---|---|")

        for i, p in enumerate(rest_papers, 4):
            title_short = p["title"][:80] # ç¨å¾®é•¿ä¸€ç‚¹
            if len(p["title"]) > 80:
                title_short += "..."
            
            title_link = f"[{title_short}]({p['url']})"
            
            # å¦‚æœæœ‰ä»£ç ï¼Œåœ¨æ ‡é¢˜ååŠ æ ‡è¯†
            if p.get("has_code"):
                title_link += f" ğŸ’»"

            reason = p.get("one_liner", "")
            cat_display = get_category_name(p.get('category', ''))
            lines.append(f"| {i} | {title_link} `{cat_display}` | {reason} |")

    # â”€â”€â”€ é¡µè„š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines.append(f"\n---\n")
    lines.append(f"ğŸ§ª ArXiv Daily Digest | æ‰«æ cs.CL / cs.AI / cs.LG | {date_str}\n")

    return "\n".join(lines)


# â”€â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run():
    """ä¸»æµç¨‹"""
    print("=" * 55)
    print("ğŸ§ª ArXiv Daily Digest â€” AI è®ºæ–‡ç²¾é€‰æ—¥æŠ¥")
    print("=" * 55)

    # 1. æŠ“å–è®ºæ–‡
    print("\n[1/5] æŠ“å– ArXiv æœ€æ–°è®ºæ–‡...")
    scraper = ArxivScraper(max_results=50)
    papers = scraper.fetch_papers()
    if not papers:
        print("âŒ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡ï¼Œæµç¨‹ç»ˆæ­¢")
        return
    total_scanned = len(papers)

    # 2. AI æ‰¹é‡ç­›é€‰
    print(f"\n[2/5] AI ä» {total_scanned} ç¯‡ä¸­ç²¾é€‰ 9 ç¯‡...")
    try:
        client = DoubaoClient()
    except ValueError as e:
        print(f"âŒ AI å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    selected, usage_select = ai_select_papers(papers, client)
    usage_select["api_calls"] = 1

    # 3. å¹¶è¡Œæ£€æŸ¥ä»£ç ä»“åº“
    print(f"\n[3/5] æ£€æŸ¥ä»£ç ä»“åº“...")
    hunter = CodeHunter(github_token=os.getenv("GITHUB_TOKEN"))
    selected = check_all_code(selected, hunter)

    # 4. Top 2 æ·±åº¦æ‘˜è¦
    print(f"\n[4/5] ä¸º Top 2 ç”Ÿæˆæ·±åº¦æ‘˜è¦...")
    usage_deep = generate_deep_summaries(selected[:2], client)

    # æ‰“å°ç”¨é‡ç»Ÿè®¡
    total_tokens = usage_select.get("total_tokens", 0) + usage_deep.get("total_tokens", 0)
    total_calls = 1 + usage_deep.get("api_calls", 0)
    if total_tokens:
        print(f"\n[API ç”¨é‡] è°ƒç”¨ {total_calls} æ¬¡ | åˆè®¡ {total_tokens:,} tokens")

    # 5. ç»„è£…æŠ¥å‘Š + å­˜å‚¨ + æ¨é€
    print(f"\n[5/5] ç»„è£…æŠ¥å‘Š â†’ å­˜å‚¨ â†’ æ¨é€...")
    report = build_report(selected, usage_select, usage_deep, total_scanned)

    # æ‰“å°æŠ¥å‘Šé¢„è§ˆ
    print("\n" + "=" * 55)
    print("ğŸ“‹ æŠ¥å‘Šé¢„è§ˆ:")
    print("=" * 55)
    print(report)
    print("=" * 55)

    save_run_data(selected, report, total_scanned)

    title = f"ğŸ§ª {datetime.now().strftime('%m-%d')} ArXiv AI æ—¥æŠ¥"
    notifier = HubNotifier()
    success = notifier.send_all(report, title)

    if success:
        print("\nâœ… æ¨é€æˆåŠŸï¼")
    else:
        print("\nâš ï¸ æ¨é€å¤±è´¥æˆ–æ— å¯ç”¨æ¸ é“ï¼Œè¯·æ£€æŸ¥ .env é…ç½®")

    print("\nğŸ ArXiv Daily Digest æµç¨‹å®Œæˆ")


if __name__ == "__main__":
    run()
